{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e346e4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n",
      "hello how are u?\n",
      "\n",
      "<class 'tokenizers.Tokenizer'>\n",
      "Héllò hôW are ü?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint_uncased = \"bert-base-uncased\"\n",
    "checkpoint_cased = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_uncased)\n",
    "tokenizer_cased = AutoTokenizer.from_pretrained(checkpoint_cased)\n",
    "\n",
    "# 1. Normalized the input\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "print(f\"{tokenizer.backend_tokenizer.normalizer.normalize_str('Héllò hôW are ü?')}\\n\")\n",
    "\n",
    "print(type(tokenizer_cased.backend_tokenizer))\n",
    "print(f\"{tokenizer_cased.backend_tokenizer.normalizer.normalize_str('Héllò hôW are ü?')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de7dc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(model_checkpoint):\n",
    "    return AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenizer_pre_tokenize_str(tokenizer, sentence):\n",
    "    return tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66ebcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bffa74f84843edba37f6e320de2147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce65d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert ->\n",
      " [('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]\n",
      "\n",
      "gpt ->\n",
      "[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)), ('?', (19, 20))]\n",
      "\n",
      "t5 ->\n",
      "[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]\n"
     ]
    }
   ],
   "source": [
    "# 2. Pre-tokenizer\n",
    "tokens_bert = tokenizer_pre_tokenize_str(tokenizer, \"Hello, how are  you?\")\n",
    "\n",
    "tokenizer_gpt = tokenizer_function(\"gpt2\")\n",
    "tokens_gpt = tokenizer_pre_tokenize_str(tokenizer_gpt, \"Hello, how are  you?\")\n",
    "\n",
    "tokenizer_t5 = tokenizer_function(\"t5-small\")\n",
    "tokens_t5 = tokenizer_pre_tokenize_str(tokenizer_t5, \"Hello, how are  you?\")\n",
    "\n",
    "print(f\"bert ->\\n {tokens_bert}\\n\\ngpt ->\\n{tokens_gpt}\\n\\nt5 ->\\n{tokens_t5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
