#!/usr/bin/env python3
"""
Moteur de recherche sÃ©mantique avec ğŸ¤— Datasets et FAISS
Solution au problÃ¨me DatasetGenerationError

BasÃ© sur le tutoriel Hugging Face : https://huggingface.co/learn/llm-course/fr/chapter5/5
"""

import requests
import time
import math
import json
from pathlib import Path
import pandas as pd
from tqdm import tqdm
from datasets import load_dataset, Dataset

def fetch_issues_clean(
    owner="huggingface",
    repo="datasets",
    num_issues=1000,  # RÃ©duisons pour Ã©viter les problÃ¨mes
    issues_path=Path("."),
):
    """
    RÃ©cupÃ¨re les issues GitHub et les nettoie pour Ã©viter les problÃ¨mes de typage.
    
    Le problÃ¨me principal Ã©tait que l'API GitHub retourne des donnÃ©es avec :
    - Des valeurs null qui causent des problÃ¨mes de typage
    - Des champs complexes imbriquÃ©s
    - Des types inconsistants entre les issues
    """
    if not issues_path.is_dir():
        issues_path.mkdir(exist_ok=True)

    all_issues = []
    per_page = 100
    num_pages = math.ceil(num_issues / per_page)
    base_url = "https://api.github.com/repos"
    headers = {}  # Pas de token pour ce test

    print(f"RÃ©cupÃ©ration de {num_issues} issues sur {num_pages} pages...")
    
    for page in tqdm(range(1, num_pages + 1)):
        query = f"issues?page={page}&per_page={per_page}&state=all"
        
        try:
            response = requests.get(f"{base_url}/{owner}/{repo}/{query}", headers=headers)
            response.raise_for_status()
            
            page_issues = response.json()
            
            if not page_issues:
                print(f"Aucune issue trouvÃ©e Ã  la page {page}")
                break
                
            # SOLUTION : Nettoyer et simplifier les donnÃ©es pour Ã©viter les problÃ¨mes de typage
            cleaned_issues = []
            for issue in page_issues:
                # Ne garder que les champs essentiels avec des types cohÃ©rents
                cleaned_issue = {
                    'url': issue.get('html_url', ''),
                    'id': issue.get('id', 0),
                    'number': issue.get('number', 0),
                    'title': issue.get('title', ''),
                    'state': issue.get('state', ''),
                    'body': issue.get('body') or '',  # IMPORTANT: Remplacer None par chaÃ®ne vide
                    'user': issue.get('user', {}).get('login', ''),
                    'labels': [label.get('name', '') for label in issue.get('labels', [])],
                    'comments': issue.get('comments', 0),
                    'created_at': issue.get('created_at', ''),
                    'updated_at': issue.get('updated_at', ''),
                    'is_pull_request': issue.get('pull_request') is not None  # BoolÃ©en simple
                }
                cleaned_issues.append(cleaned_issue)
            
            all_issues.extend(cleaned_issues)
            
            # Pause pour respecter les limites de l'API
            time.sleep(0.5)
            
            if len(all_issues) >= num_issues:
                break
                
        except requests.exceptions.RequestException as e:
            print(f"Erreur lors de la requÃªte page {page}: {e}")
            break

    # Limiter au nombre demandÃ©
    all_issues = all_issues[:num_issues]
    
    # Sauvegarder avec un nom diffÃ©rent pour Ã©viter les conflits
    output_file = issues_path / f"{repo}-issues-cleaned.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for issue in all_issues:
            json.dump(issue, f, ensure_ascii=False)
            f.write('\n')
    
    print(f"âœ… {len(all_issues)} issues sauvegardÃ©es dans {output_file}")
    return output_file

def clean_issues(dataset):
    """Nettoie le dataset des issues GitHub selon le tutoriel."""
    print(f"Dataset initial: {len(dataset)} issues")
    
    # Filtrer les pull requests
    dataset = dataset.filter(lambda x: not x["is_pull_request"])
    print(f"AprÃ¨s filtrage des PR: {len(dataset)} issues")
    
    # Filtrer les issues sans corps ou trop courtes
    dataset = dataset.filter(lambda x: x["body"] is not None and len(x["body"]) > 15)
    print(f"AprÃ¨s filtrage des issues vides: {len(dataset)} issues")
    
    # CrÃ©er une colonne de texte combinÃ©
    def create_text(example):
        title = example["title"] if example["title"] else ""
        body = example["body"] if example["body"] else ""
        return {"text": f"{title}\n\n{body}"}
    
    dataset = dataset.map(create_text)
    print(f"Texte combinÃ© crÃ©Ã© pour {len(dataset)} issues")
    
    return dataset

def main():
    """Fonction principale pour tester la solution."""
    print("ğŸ”§ Solution au problÃ¨me DatasetGenerationError")
    print("=" * 50)
    
    # 1. RÃ©cupÃ©rer et nettoyer les donnÃ©es
    print("\n1. RÃ©cupÃ©ration des issues...")
    issues_file = fetch_issues_clean()
    
    # 2. Charger le dataset
    print("\n2. Chargement du dataset...")
    try:
        issues_dataset = load_dataset('json', data_files=str(issues_file), split='train')
        print(f"âœ… Dataset chargÃ© avec succÃ¨s!")
        print(f"Nombre d'issues: {len(issues_dataset)}")
        print(f"Colonnes: {issues_dataset.column_names}")
    except Exception as e:
        print(f"âŒ Erreur lors du chargement: {e}")
        return
    
    # 3. Examiner un Ã©chantillon
    print("\n3. Examen d'un Ã©chantillon...")
    if len(issues_dataset) > 0:
        sample = issues_dataset[0]
        print("PremiÃ¨re issue:")
        print(f"Titre: {sample['title']}")
        print(f"Ã‰tat: {sample['state']}")
        print(f"Utilisateur: {sample['user']}")
        print(f"Est une PR: {sample['is_pull_request']}")
        print(f"Corps (premiers 200 caractÃ¨res): {sample['body'][:200]}...")
    
    # 4. Nettoyer selon le tutoriel
    print("\n4. Nettoyage des donnÃ©es...")
    issues_dataset = clean_issues(issues_dataset)
    
    # 5. Examiner le rÃ©sultat final
    print("\n5. RÃ©sultat final...")
    if len(issues_dataset) > 0:
        sample = issues_dataset[0]
        print("Texte de la premiÃ¨re issue (premiers 300 caractÃ¨res):")
        print(sample['text'][:300] + "...")
    
    print(f"\nâœ… SuccÃ¨s! Dataset final: {len(issues_dataset)} issues prÃªtes pour la recherche sÃ©mantique")

if __name__ == "__main__":
    main()
