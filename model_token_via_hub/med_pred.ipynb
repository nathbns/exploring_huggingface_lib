{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573508c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    load_from_disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d788bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\") \n",
    "# drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7daa3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 27703/27703 [00:01<00:00, 17953.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes après tokenisation: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Exemple de labels: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Préparation des labels : convertir les ratings en indices de classe\n",
    "# Les ratings vont de 1 à 10, nous les convertissons en indices 0-9\n",
    "def prepare_labels(example):\n",
    "    # Convertir le rating en indice de classe (rating 1.0 -> classe 0, rating 10.0 -> classe 9)\n",
    "    example['labels'] = int(example['rating']) - 1\n",
    "    return example\n",
    "\n",
    "# Appliquer la préparation des labels avant tokenisation\n",
    "drug_dataset_with_labels = drug_dataset_reloaded.map(prepare_labels)\n",
    "\n",
    "def tokenize_and_split(example):\n",
    "    result = tokenizer(\n",
    "        example['review'],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    # Extraire la correspondance entre les nouveaux et les anciens indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in example.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = drug_dataset_with_labels.map(tokenize_and_split, batched=True)\n",
    "print(\"Colonnes après tokenisation:\", tokenized_dataset[\"train\"].column_names)\n",
    "print(\"Exemple de labels:\", tokenized_dataset[\"train\"][0][\"labels\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ff1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle configuré avec 10 classes\n",
      "Configuration d'entraînement optimisée pour ~10-15 minutes au lieu de 3h\n"
     ]
    }
   ],
   "source": [
    "# Configuration optimisée pour un entraînement rapide\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_med\",\n",
    "    num_train_epochs=1,              # 1 époque au lieu de 3 par défaut\n",
    "    per_device_train_batch_size=32,  # Batch size plus grand pour aller plus vite\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=100,                # Moins de warmup steps\n",
    "    max_steps=500,                   # Limiter le nombre de steps total\n",
    ")\n",
    "\n",
    "# Configurer le modèle avec le bon nombre de classes (ratings de 1 à 10 = 10 classes)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=10  # 10 classes pour les ratings de 1 à 10\n",
    ")\n",
    "print(f\"Modèle configuré avec {model.num_labels} classes\")\n",
    "print(f\"Configuration d'entraînement optimisée pour ~10-15 minutes au lieu de 3h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a17585af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset d'entraînement : 5000\n",
      "Taille du dataset de validation : 1000\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Utiliser un sous-ensemble pour un entraînement ultra-rapide (2-3 minutes)\n",
    "# Décommentez ces lignes pour un test vraiment rapide\n",
    "train_subset = tokenized_dataset[\"train\"].select(range(5000))  # 5k exemples au lieu de 110k\n",
    "eval_subset = tokenized_dataset[\"validation\"].select(range(1000))  # 1k exemples au lieu de 27k\n",
    "\n",
    "# Option 2: Utiliser le dataset complet (plus long mais meilleur résultat)\n",
    "# train_subset = tokenized_dataset[\"train\"]\n",
    "# eval_subset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "print(f\"Taille du dataset d'entraînement : {len(train_subset)}\")\n",
    "print(f\"Taille du dataset de validation : {len(eval_subset)}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=eval_subset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c00834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de classes du modèle : 10\n",
      "Colonnes du dataset : ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Exemple de labels dans le dataset : 1\n",
      "Type des labels : <class 'int'>\n",
      "Labels uniques dans l'échantillon : [0 1 2 3 4 5 6 7 8 9]\n",
      "Range des labels : min=0, max=9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Nombre de classes du modèle : {model.num_labels}\")\n",
    "print(f\"Colonnes du dataset : {tokenized_dataset['train'].column_names}\")\n",
    "print(f\"Exemple de labels dans le dataset : {tokenized_dataset['train'][0]['labels']}\")\n",
    "print(f\"Type des labels : {type(tokenized_dataset['train'][0]['labels'])}\")\n",
    "\n",
    "import numpy as np\n",
    "sample_labels = [tokenized_dataset['train'][i]['labels'] for i in range(100)]\n",
    "unique_labels = np.unique(sample_labels)\n",
    "print(f\"Labels uniques dans l'échantillon : {unique_labels}\")\n",
    "print(f\"Range des labels : min={min(sample_labels)}, max={max(sample_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d5ac7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure du dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 110811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 27703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n",
      "\n",
      "Exemple du dataset d'entraînement:\n",
      "{'patient_id': 89879, 'drugName': 'Cyclosporine', 'condition': 'keratoconjunctivitis sicca', 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"', 'rating': 2.0, 'date': 'April 20, 2013', 'usefulCount': 69, 'review_length': 147}\n",
      "\n",
      "Colonnes disponibles: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n"
     ]
    }
   ],
   "source": [
    "print(\"Structure du dataset:\")\n",
    "print(drug_dataset_reloaded)\n",
    "print(\"\\nExemple du dataset d'entraînement:\")\n",
    "print(drug_dataset_reloaded[\"train\"][0])\n",
    "print(\"\\nColonnes disponibles:\", drug_dataset_reloaded[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2683da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 05:46, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.767600</td>\n",
       "      <td>1.799052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.348700</td>\n",
       "      <td>1.826062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nath/Desktop/huggingFace_LLM2/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nath/Desktop/huggingFace_LLM2/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nath/Desktop/huggingFace_LLM2/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.648895965576172, metrics={'train_runtime': 347.6231, 'train_samples_per_second': 46.027, 'train_steps_per_second': 1.438, 'total_flos': 1047783478038528.0, 'train_loss': 1.648895965576172, 'epoch': 3.1847133757961785})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÉVALUATION ET TEST DU MODÈLE =====\n",
    "# Exécutez ces cellules après l'entraînement\n",
    "\n",
    "# 1. Évaluation sur le dataset de test\n",
    "print(\"🔍 Évaluation du modèle sur le dataset de test...\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"].select(range(1000)))  # 1k échantillons pour plus rapide\n",
    "print(\"Résultats sur le dataset de test:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad09109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fonction pour faire des prédictions sur de nouveaux textes\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_rating(text, model, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Prédit le rating (1-10) pour un texte donné\n",
    "    \"\"\"\n",
    "    # Tokeniser le texte\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Faire la prédiction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0, predicted_class].item()\n",
    "    \n",
    "    # Convertir l'indice de classe en rating (0-9 -> 1-10)\n",
    "    predicted_rating = predicted_class + 1\n",
    "    \n",
    "    return {\n",
    "        'rating_prédit': predicted_rating,\n",
    "        'confiance': confidence,\n",
    "        'probabilités_par_classe': probabilities[0].tolist()\n",
    "    }\n",
    "\n",
    "print(\"✅ Fonction de prédiction créée !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test avec des exemples concrets\n",
    "print(\"Test du modèle avec des exemples concrets...\")\n",
    "\n",
    "# Exemples de reviews avec différents sentiments\n",
    "exemples_test = [\n",
    "    {\n",
    "        \"text\": \"This medication is absolutely amazing! It completely cured my condition with no side effects. I feel like a new person!\",\n",
    "        \"sentiment_attendu\": \"Très positif (rating élevé)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Terrible drug, made me feel worse than before. Severe side effects and no improvement at all. Waste of money.\",\n",
    "        \"sentiment_attendu\": \"Très négatif (rating faible)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The medication works okay, some improvement but also some minor side effects. Could be better.\",\n",
    "        \"sentiment_attendu\": \"Neutre/moyen (rating moyen)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Good results overall. The drug helped significantly with my symptoms. Minor side effects but manageable.\",\n",
    "        \"sentiment_attendu\": \"Positif (rating bon)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Prédictions sur les exemples de test:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, exemple in enumerate(exemples_test, 1):\n",
    "    print(f\"\\n Exemple {i}:\")\n",
    "    print(f\"Texte: {exemple['text']}\")\n",
    "    print(f\"Sentiment attendu: {exemple['sentiment_attendu']}\")\n",
    "    \n",
    "    prediction = predict_rating(exemple['text'], model, tokenizer)\n",
    "    print(f\"Rating prédit: {prediction['rating_prédit']}/10\")\n",
    "    print(f\"Confiance: {prediction['confiance']:.3f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analyse détaillée des performances\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_detailed(model, tokenizer, test_dataset, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Évaluation détaillée avec métriques complètes\n",
    "    \"\"\"\n",
    "    print(\" Analyse détaillée des performances...\")\n",
    "    \n",
    "    # Prendre un échantillon du dataset de test\n",
    "    test_sample = test_dataset.select(range(min(max_samples, len(test_dataset))))\n",
    "    \n",
    "    # Prédictions\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for example in test_sample:\n",
    "        pred = predict_rating(example['review'], model, tokenizer)\n",
    "        predictions.append(pred['rating_prédit'] - 1)  # Convertir en 0-9\n",
    "        true_labels.append(example['labels'])\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\" Résultats sur {len(test_sample)} échantillons:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Accuracy en %: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Rapport de classification détaillé:\")\n",
    "    class_names = [f\"Rating {i+1}\" for i in range(10)]\n",
    "    print(classification_report(true_labels, predictions, \n",
    "                              target_names=class_names, \n",
    "                              zero_division=0))\n",
    "    \n",
    "    return accuracy, predictions, true_labels\n",
    "\n",
    "# Exécuter l'évaluation détaillée (à faire après l'entraînement)\n",
    "# accuracy, preds, labels = evaluate_detailed(model, tokenizer, tokenized_dataset[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89360d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test interactif du modèle\n",
      "Entrez vos propres reviews de médicaments pour voir les prédictions!\n",
      "(Tapez 'quit' pour arrêter)\n",
      "------------------------------------------------------------\n",
      " Erreur: name 'predict_rating' is not defined\n",
      " Erreur: name 'predict_rating' is not defined\n",
      " Aurevoir !\n"
     ]
    }
   ],
   "source": [
    "# 5. Test interactif - entrez votre propre texte !\n",
    "def test_interactif():\n",
    "    \"\"\"\n",
    "    Fonction pour tester le modèle avec vos propres textes\n",
    "    \"\"\"\n",
    "    print(\" Test interactif du modèle\")\n",
    "    print(\"Entrez vos propres reviews de médicaments pour voir les prédictions!\")\n",
    "    print(\"(Tapez 'quit' pour arrêter)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n Entrez votre review: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\" Aurevoir !\")\n",
    "            break\n",
    "            \n",
    "        if user_input.strip():\n",
    "            try:\n",
    "                prediction = predict_rating(user_input, model, tokenizer)\n",
    "                print(f\" Rating prédit: {prediction['rating_prédit']}/10\")\n",
    "                print(f\" Confiance: {prediction['confiance']:.3f}\")\n",
    "                \n",
    "                # Interpretation du rating\n",
    "                rating = prediction['rating_prédit']\n",
    "                if rating >= 8:\n",
    "                    interpretation = \"😍 Très satisfait\"\n",
    "                elif rating >= 6:\n",
    "                    interpretation = \"😊 Satisfait\" \n",
    "                elif rating >= 4:\n",
    "                    interpretation = \"😐 Neutre\"\n",
    "                else:\n",
    "                    interpretation = \"😞 Pas satisfait\"\n",
    "                    \n",
    "                print(f\"Interprétation: {interpretation}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Erreur: {e}\")\n",
    "        else:\n",
    "            print(\"Veuillez entrer un texte non vide\")\n",
    "\n",
    "# Pour lancer le test interactif, décommentez la ligne suivante après l'entraînement:\n",
    "test_interactif() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
