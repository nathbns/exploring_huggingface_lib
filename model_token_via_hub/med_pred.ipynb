{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573508c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    load_from_disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d788bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\") \n",
    "# drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7daa3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27703/27703 [00:01<00:00, 17953.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes apr√®s tokenisation: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Exemple de labels: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Pr√©paration des labels : convertir les ratings en indices de classe\n",
    "# Les ratings vont de 1 √† 10, nous les convertissons en indices 0-9\n",
    "def prepare_labels(example):\n",
    "    # Convertir le rating en indice de classe (rating 1.0 -> classe 0, rating 10.0 -> classe 9)\n",
    "    example['labels'] = int(example['rating']) - 1\n",
    "    return example\n",
    "\n",
    "# Appliquer la pr√©paration des labels avant tokenisation\n",
    "drug_dataset_with_labels = drug_dataset_reloaded.map(prepare_labels)\n",
    "\n",
    "def tokenize_and_split(example):\n",
    "    result = tokenizer(\n",
    "        example['review'],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "    # Extraire la correspondance entre les nouveaux et les anciens indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in example.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = drug_dataset_with_labels.map(tokenize_and_split, batched=True)\n",
    "print(\"Colonnes apr√®s tokenisation:\", tokenized_dataset[\"train\"].column_names)\n",
    "print(\"Exemple de labels:\", tokenized_dataset[\"train\"][0][\"labels\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ff1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le configur√© avec 10 classes\n",
      "Configuration d'entra√Ænement optimis√©e pour ~10-15 minutes au lieu de 3h\n"
     ]
    }
   ],
   "source": [
    "# Configuration optimis√©e pour un entra√Ænement rapide\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_med\",\n",
    "    num_train_epochs=1,              # 1 √©poque au lieu de 3 par d√©faut\n",
    "    per_device_train_batch_size=32,  # Batch size plus grand pour aller plus vite\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=100,                # Moins de warmup steps\n",
    "    max_steps=500,                   # Limiter le nombre de steps total\n",
    ")\n",
    "\n",
    "# Configurer le mod√®le avec le bon nombre de classes (ratings de 1 √† 10 = 10 classes)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=10  # 10 classes pour les ratings de 1 √† 10\n",
    ")\n",
    "print(f\"Mod√®le configur√© avec {model.num_labels} classes\")\n",
    "print(f\"Configuration d'entra√Ænement optimis√©e pour ~10-15 minutes au lieu de 3h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a17585af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset d'entra√Ænement : 5000\n",
      "Taille du dataset de validation : 1000\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Utiliser un sous-ensemble pour un entra√Ænement ultra-rapide (2-3 minutes)\n",
    "# D√©commentez ces lignes pour un test vraiment rapide\n",
    "train_subset = tokenized_dataset[\"train\"].select(range(5000))  # 5k exemples au lieu de 110k\n",
    "eval_subset = tokenized_dataset[\"validation\"].select(range(1000))  # 1k exemples au lieu de 27k\n",
    "\n",
    "# Option 2: Utiliser le dataset complet (plus long mais meilleur r√©sultat)\n",
    "# train_subset = tokenized_dataset[\"train\"]\n",
    "# eval_subset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "print(f\"Taille du dataset d'entra√Ænement : {len(train_subset)}\")\n",
    "print(f\"Taille du dataset de validation : {len(eval_subset)}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=eval_subset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c00834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de classes du mod√®le : 10\n",
      "Colonnes du dataset : ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Exemple de labels dans le dataset : 1\n",
      "Type des labels : <class 'int'>\n",
      "Labels uniques dans l'√©chantillon : [0 1 2 3 4 5 6 7 8 9]\n",
      "Range des labels : min=0, max=9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Nombre de classes du mod√®le : {model.num_labels}\")\n",
    "print(f\"Colonnes du dataset : {tokenized_dataset['train'].column_names}\")\n",
    "print(f\"Exemple de labels dans le dataset : {tokenized_dataset['train'][0]['labels']}\")\n",
    "print(f\"Type des labels : {type(tokenized_dataset['train'][0]['labels'])}\")\n",
    "\n",
    "import numpy as np\n",
    "sample_labels = [tokenized_dataset['train'][i]['labels'] for i in range(100)]\n",
    "unique_labels = np.unique(sample_labels)\n",
    "print(f\"Labels uniques dans l'√©chantillon : {unique_labels}\")\n",
    "print(f\"Range des labels : min={min(sample_labels)}, max={max(sample_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d5ac7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure du dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 110811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 27703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n",
      "\n",
      "Exemple du dataset d'entra√Ænement:\n",
      "{'patient_id': 89879, 'drugName': 'Cyclosporine', 'condition': 'keratoconjunctivitis sicca', 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"', 'rating': 2.0, 'date': 'April 20, 2013', 'usefulCount': 69, 'review_length': 147}\n",
      "\n",
      "Colonnes disponibles: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n"
     ]
    }
   ],
   "source": [
    "print(\"Structure du dataset:\")\n",
    "print(drug_dataset_reloaded)\n",
    "print(\"\\nExemple du dataset d'entra√Ænement:\")\n",
    "print(drug_dataset_reloaded[\"train\"][0])\n",
    "print(\"\\nColonnes disponibles:\", drug_dataset_reloaded[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2683da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 05:46, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.767600</td>\n",
       "      <td>1.799052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.348700</td>\n",
       "      <td>1.826062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nath/Desktop/huggingFace_LLM2/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nath/Desktop/huggingFace_LLM2/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nath/Desktop/huggingFace_LLM2/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.648895965576172, metrics={'train_runtime': 347.6231, 'train_samples_per_second': 46.027, 'train_steps_per_second': 1.438, 'total_flos': 1047783478038528.0, 'train_loss': 1.648895965576172, 'epoch': 3.1847133757961785})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== √âVALUATION ET TEST DU MOD√àLE =====\n",
    "# Ex√©cutez ces cellules apr√®s l'entra√Ænement\n",
    "\n",
    "# 1. √âvaluation sur le dataset de test\n",
    "print(\"üîç √âvaluation du mod√®le sur le dataset de test...\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"].select(range(1000)))  # 1k √©chantillons pour plus rapide\n",
    "print(\"R√©sultats sur le dataset de test:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad09109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fonction pour faire des pr√©dictions sur de nouveaux textes\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_rating(text, model, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Pr√©dit le rating (1-10) pour un texte donn√©\n",
    "    \"\"\"\n",
    "    # Tokeniser le texte\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Faire la pr√©diction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0, predicted_class].item()\n",
    "    \n",
    "    # Convertir l'indice de classe en rating (0-9 -> 1-10)\n",
    "    predicted_rating = predicted_class + 1\n",
    "    \n",
    "    return {\n",
    "        'rating_pr√©dit': predicted_rating,\n",
    "        'confiance': confidence,\n",
    "        'probabilit√©s_par_classe': probabilities[0].tolist()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Fonction de pr√©diction cr√©√©e !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test avec des exemples concrets\n",
    "print(\"Test du mod√®le avec des exemples concrets...\")\n",
    "\n",
    "# Exemples de reviews avec diff√©rents sentiments\n",
    "exemples_test = [\n",
    "    {\n",
    "        \"text\": \"This medication is absolutely amazing! It completely cured my condition with no side effects. I feel like a new person!\",\n",
    "        \"sentiment_attendu\": \"Tr√®s positif (rating √©lev√©)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Terrible drug, made me feel worse than before. Severe side effects and no improvement at all. Waste of money.\",\n",
    "        \"sentiment_attendu\": \"Tr√®s n√©gatif (rating faible)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The medication works okay, some improvement but also some minor side effects. Could be better.\",\n",
    "        \"sentiment_attendu\": \"Neutre/moyen (rating moyen)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Good results overall. The drug helped significantly with my symptoms. Minor side effects but manageable.\",\n",
    "        \"sentiment_attendu\": \"Positif (rating bon)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Pr√©dictions sur les exemples de test:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, exemple in enumerate(exemples_test, 1):\n",
    "    print(f\"\\n Exemple {i}:\")\n",
    "    print(f\"Texte: {exemple['text']}\")\n",
    "    print(f\"Sentiment attendu: {exemple['sentiment_attendu']}\")\n",
    "    \n",
    "    prediction = predict_rating(exemple['text'], model, tokenizer)\n",
    "    print(f\"Rating pr√©dit: {prediction['rating_pr√©dit']}/10\")\n",
    "    print(f\"Confiance: {prediction['confiance']:.3f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analyse d√©taill√©e des performances\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_detailed(model, tokenizer, test_dataset, max_samples=1000):\n",
    "    \"\"\"\n",
    "    √âvaluation d√©taill√©e avec m√©triques compl√®tes\n",
    "    \"\"\"\n",
    "    print(\" Analyse d√©taill√©e des performances...\")\n",
    "    \n",
    "    # Prendre un √©chantillon du dataset de test\n",
    "    test_sample = test_dataset.select(range(min(max_samples, len(test_dataset))))\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for example in test_sample:\n",
    "        pred = predict_rating(example['review'], model, tokenizer)\n",
    "        predictions.append(pred['rating_pr√©dit'] - 1)  # Convertir en 0-9\n",
    "        true_labels.append(example['labels'])\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\" R√©sultats sur {len(test_sample)} √©chantillons:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Accuracy en %: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Rapport de classification d√©taill√©:\")\n",
    "    class_names = [f\"Rating {i+1}\" for i in range(10)]\n",
    "    print(classification_report(true_labels, predictions, \n",
    "                              target_names=class_names, \n",
    "                              zero_division=0))\n",
    "    \n",
    "    return accuracy, predictions, true_labels\n",
    "\n",
    "# Ex√©cuter l'√©valuation d√©taill√©e (√† faire apr√®s l'entra√Ænement)\n",
    "# accuracy, preds, labels = evaluate_detailed(model, tokenizer, tokenized_dataset[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89360d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test interactif du mod√®le\n",
      "Entrez vos propres reviews de m√©dicaments pour voir les pr√©dictions!\n",
      "(Tapez 'quit' pour arr√™ter)\n",
      "------------------------------------------------------------\n",
      " Erreur: name 'predict_rating' is not defined\n",
      " Erreur: name 'predict_rating' is not defined\n",
      " Aurevoir !\n"
     ]
    }
   ],
   "source": [
    "# 5. Test interactif - entrez votre propre texte !\n",
    "def test_interactif():\n",
    "    \"\"\"\n",
    "    Fonction pour tester le mod√®le avec vos propres textes\n",
    "    \"\"\"\n",
    "    print(\" Test interactif du mod√®le\")\n",
    "    print(\"Entrez vos propres reviews de m√©dicaments pour voir les pr√©dictions!\")\n",
    "    print(\"(Tapez 'quit' pour arr√™ter)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n Entrez votre review: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\" Aurevoir !\")\n",
    "            break\n",
    "            \n",
    "        if user_input.strip():\n",
    "            try:\n",
    "                prediction = predict_rating(user_input, model, tokenizer)\n",
    "                print(f\" Rating pr√©dit: {prediction['rating_pr√©dit']}/10\")\n",
    "                print(f\" Confiance: {prediction['confiance']:.3f}\")\n",
    "                \n",
    "                # Interpretation du rating\n",
    "                rating = prediction['rating_pr√©dit']\n",
    "                if rating >= 8:\n",
    "                    interpretation = \"üòç Tr√®s satisfait\"\n",
    "                elif rating >= 6:\n",
    "                    interpretation = \"üòä Satisfait\" \n",
    "                elif rating >= 4:\n",
    "                    interpretation = \"üòê Neutre\"\n",
    "                else:\n",
    "                    interpretation = \"üòû Pas satisfait\"\n",
    "                    \n",
    "                print(f\"Interpr√©tation: {interpretation}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Erreur: {e}\")\n",
    "        else:\n",
    "            print(\"Veuillez entrer un texte non vide\")\n",
    "\n",
    "# Pour lancer le test interactif, d√©commentez la ligne suivante apr√®s l'entra√Ænement:\n",
    "test_interactif() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
