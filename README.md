# Exploring Hugging Face Library

A comprehensive exploration of Hugging Face's ecosystem including transformers, tokenizers, datasets, and fine-tuning capabilities.

## Contents

- **Transformers API** - Pipeline usage, model initialization, tokenization, and padding
- **Fine-tuning** - Custom model training with Trainer API and from-scratch implementations  
- **Datasets API** - Data loading, processing, and collection strategies
- **Tokenizers** - Custom tokenizer creation and configuration
- **FAISS Search** - Semantic similarity search implementation
- **Large Datasets** - Handling and processing large-scale data

## Quick Start

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Explore the notebooks in each directory to learn different aspects of the Hugging Face ecosystem.

## Project Structure

- `transformers_api/` - Core transformers functionality and pipelines
- `fine_tuning/` - Model training examples and checkpoints
- `datasets_api/` - Dataset manipulation and data collection
- `tokenizer/` - Custom tokenizer implementations
- `faiss_search/` - Similarity search with FAISS
- `very_large_dataset/` - Large-scale data processing examples
- `model_token_via_hub/` - Model and tokenizer usage via Hugging Face Hub

## üõ†Ô∏è Technologies

- Transformers
- PyTorch  
- Datasets
- FAISS
- Pandas & NumPy
