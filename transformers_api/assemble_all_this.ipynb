{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98329b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6e6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc0b624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Remplit les séquences jusqu\\'à la longueur maximale de la séquence\\nmodel_inputs = tokenizer(sequences, padding=\"longest\")\\n\\n# Remplit les séquences jusqu\\'à la longueur maximale du modèle (512 pour BERT ou DistilBERT)\\nmodel_inputs = tokenizer(sequences, padding=\"max_length\")\\n\\n# Remplit les séquences jusqu\\'à la longueur maximale spécifiée\\nmodel_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding possible:\n",
    "\"\"\"\n",
    "# Remplit les séquences jusqu'à la longueur maximale de la séquence\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Remplit les séquences jusqu'à la longueur maximale du modèle (512 pour BERT ou DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Remplit les séquences jusqu'à la longueur maximale spécifiée\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57db4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "# Tronque les séquences qui sont plus longues que la longueur maximale du modèle\n",
    "# (512 pour BERT ou DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Tronque les séquences qui sont plus longues que la longueur maximale spécifiée\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6203e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_input['input_ids'] = [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "decode = '[CLS] i've been waiting for a huggingface course my whole life. [SEP]'\n",
      "\n",
      "ids = [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "decode = 'i've been waiting for a huggingface course my whole life.'\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_input = tokenizer(sequence)\n",
    "print(f\"model_input['input_ids'] = {model_input['input_ids']}\")\n",
    "print(f\"decode = '{tokenizer.decode(model_input['input_ids'])}'\\n\")\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"ids = {ids}\")\n",
    "print(f\"decode = '{tokenizer.decode(ids)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2fd540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  voyons une dernière fois comment il peut gérer plusieurs séquences (padding), \n",
    "# de très longues séquences (troncation) \n",
    "# et plusieurs types de tenseurs avec son API principale \n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Le double astérisque (**) permet de décomposer le dictionnaire 'tokens' en arguments nommés pour la fonction 'model'.\n",
    "# Par exemple, si tokens = {'input_ids': ..., 'attention_mask': ...}, alors model(**tokens) équivaut à model(input_ids=..., attention_mask=...)\n",
    "# output = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"])\n",
    "output = model(**tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
